<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Zizhang Li</title>
  
  <meta name="author" content="Zizhang Li">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/x-ico" href="/images/favicon.ico">
  <link rel="shortcut icon" type="image/x-ico" href="/images/favicon.ico">
  <link rel="apple-touch-icon" type="image/x-ico" href="/images/favicon.ico">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Zizhang Li</name>
              </p>
              <p>
                I am a CS Ph.D. student at <a href="https://cs.stanford.edu/">Stanford</a> starting 2024 Fall, advised by <a href="https://cs.stanford.edu/~yzzhang/">Jiajun Wu</a>. I am supported by the <a href="https://vpge.stanford.edu/fellowships-funding/sgf/details">Stanford Graduate Fellowship</a>.
              </p>
              <p>I got my M.Eng in Control Science and Engineering department from <a href="https://www.zju.edu.cn/">Zhejiang University</a> in 2024, where I was
                advised by <a href="https://person.zju.edu.cn/yongliu">Prof. Yong Liu</a> in <a href="https://april.zju.edu.cn/">April Lab</a>.
                I obtained my B.Eng from the same department with an honor degree at Chu Kochen Honor College in 2021.
              </p>
              <p>
                In past years, I had the pleasure of collaborating with <a href="https://elliottwu.com/">Shangzhe Wu</a> and <a href="https://jiajunwu.com/">Prof. Jiajun Wu</a> at Stanford University,
                <a href="https://yiyiliao.github.io/">Prof. Yiyi Liao</a> at Zhejiang University, <a href="https://jifengdai.org/">Prof. Jifeng Dai</a> at Tsinghua University, 
                <a href="https://weichaoqiu.com/">Weichao Qiu</a> and <a href="https://www.cs.jhu.edu/~ayuille/">Prof. Alan Yuille</a> at <a href="https://ccvl.jhu.edu/team/">CCVL</a>.
              </p>
              <p style="text-align:center">
                <a href="mailto:zzli@cs.stanford.edu">Email</a> &nbsp/&nbsp
                <!-- <a href="images/CV_ZizhangLi.pdf">CV</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?hl=en&user=uVtphHMAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/kyleleey/">Github</a> &nbsp/&nbsp
                <a href="https://twitter.com/zizhang_li">Twitter</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/zizhang-li-64629b2a4/">LinkedIn</a>
              </p>
            </td>
            <td style="padding:5%;width:30%;max-width:30%">
              <a href="images/Zizhang_Li.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/Zizhang_Li.jpg" class="hoverZoomLink"></a>
              <p style="text-align:center">
                Also check out: <a href="https://xukechun.github.io/">Kechun Xu</a>.
              </p>
            </td>
          </tr>
        </tbody></table>

        <hr>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
              <td style="padding:10px;width:100%;vertical-align:middle">
                  <heading>Recent News</heading>
              </td>
          </tr>
        </tbody></table>

        <!-- <div style="height:170px;overflow-y:scroll;"> -->
        <div style="height:auto;">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <ul>
            <li>[2024.04] &nbsp I am honored to receive the <b><font color="red">Stanford Graduate Fellowship Award</font></b>.</li>
          </ul>
        </td>
      </tr>
        <tr>
          <ul>
            <li>[2024.03] &nbsp I will be joining Stanford University as a CS PhD student.</li>
          </ul>
        </td>
      </tr>
        </tbody></table>
      </div>
        
        <hr>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:10px;width:100%;vertical-align:middle">
            <heading>Publications</heading>
          </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <!-- <ul> -->
            <!-- My current research interests lie in 3D reconstruction, neural rendering and computer vision. I'm particularly interested in inferring the 3D properties of object/scene from 2D image collections.  -->
            <!-- Representative papers are <span class="highlight">highlighted</span>. -->
          <!-- </ul> -->
          <ul>
            * indicates equal contributions, &#8224 indicates equal advising.
          </ul>
        </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
              <td style="padding:20px;width:35%;vertical-align:middle">
                <img align='middle' src="images/2024_sclg_Zhang.gif" alt="SCLG" style="width:auto; height:auto; max-width:80%;">
              </td>
              <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2410.16770"> <!--TODO add link-->
                      <papertitle>The Scene Language: Representing Scenes with Programs, Words, and Embeddings</papertitle>
                  </a>
                  <br>
                  <a href="https://cs.stanford.edu/~yzzhang/">Yunzhi Zhang</a>,
                  <strong>Zizhang Li</strong>,
                  Matt Zhou,
                  <a href="https://elliottwu.com/">Shangzhe Wu</a>,
                  <a href="https://jiajunwu.com/">Jiajun Wu</a>
                  <br>
                  <em>CVPR</em>, 2025,
                    <font color="red">Highlight</font>
                  <br>
                  <a href="https://ai.stanford.edu/~yzzhang/projects/scene-language/">Project page</a> /
                  <a href="https://arxiv.org/abs/2410.16770">arXiv</a> /
                  <a href="https://github.com/zzyunzhi/scene-language">Code</a>
                  <p></p>
                  <p>
                    The Scene Language is a visual scene representation that concisely and precisely describes the structure, semantics, and identity of visual scenes. 
                    It represents a scene with three key components: a program that specifies the hierarchical and relational structure of entities in the scene, 
                    words in natural language that summarize the semantic class of each entity, and embeddings that capture the visual identity of each entity. 
                  </p>
              </td>
          </tr> <!--zhang2024sclg-->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
              <td style="padding:20px;width:35%;vertical-align:middle;">
                <img align='middle' src="images/2024_3D_congealing_Zhang.gif" alt="3D-Congealing" style="width:auto; height:auto; max-width:80%;">
              </td>
              <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2404.02125"> <!--TODO add link-->
                      <papertitle>3D Congealing: 3D-Aware Image Alignment in the Wild</papertitle>
                  </a>
                  <br>
                  <a href="https://cs.stanford.edu/~yzzhang/">Yunzhi Zhang</a>,
                  <strong>Zizhang Li</strong>,
                  <a href="https://amitraj93.github.io/">Amit Raj</a>,
                  <a href="https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/computergrafik/lehrstuhl/mitarbeiter/andreas-engelhardt/">Andreas Engelhardt</a>,
                  <a href="https://people.csail.mit.edu/yzli/">Yuanzhen Li</a>,
                  <a href="https://scholar.google.com/citations?user=u-UDZcsAAAAJ&hl=en">Tingbo Hou</a>,
                  <a href="https://jiajunwu.com/">Jiajun Wu</a>,
                  <a href="https://varunjampani.github.io/">Varun Jampani</a>
                  <br>
                  <em>ECCV</em>, 2024
                  <br>
                  <a href="https://ai.stanford.edu/~yzzhang/projects/3d-congealing/">Project page</a> /
                  <a href="https://arxiv.org/abs/2404.02125">arXiv</a>
                  <p></p>
                  <p>
                    3D Congealing aligns semantically similar objects in an unposed 2D image collection to a canonical 3D representation, via fusing prior knowledge from a pre-trained image generative model and semantic information from input images.
                  </p>
              </td>
          </tr> <!--zhang20243DCongealing-->
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
              <td style="padding:20px;width:35%;vertical-align:middle">
                <img align='middle' src="images/2024Fauna_Li.gif" alt="3D-Fauna" style="width:auto; height:auto; max-width:100%;">
                <!-- <video autoplay loop muted playsinline width="320" height="100%" style="max-width: 100%;">
                  <source src="images/20230Fauna_Li.mp4" type="video/mp4" alt="3D-Fauna">
                </video> -->
              </td>
              <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2401.02400"> <!--TODO add link-->
                      <papertitle>Learning the 3D Fauna of the Web</papertitle>
                  </a>
                  <br>
                  <strong>Zizhang Li*</strong>,
                  <a href="https://dorlitvak.github.io/">Dor Litvak*</a>, 
                  <a href="https://ruiningli.com/">Ruining Li</a>,
                  <a href="https://cs.stanford.edu/~yzzhang/">Yunzhi Zhang</a>, 
                  <a href="https://www.robots.ox.ac.uk/~tomj/">Tomas Jakab</a>, 
                  <a href="https://chrirupp.github.io/">Christian Rupprecht</a>, 
                  <a href="https://elliottwu.com/">Shangzhe Wu&#8224</a>, 
                  <a href="https://www.robots.ox.ac.uk/~vedaldi/">Andrea Vedaldi&#8224</a>, 
                  <a href="https://jiajunwu.com/">Jiajun Wu&#8224</a>
                  <br>
                  <em>CVPR</em>, 2024
                  <br>
                  <a href="https://kyleleey.github.io/3DFauna/">Project page</a> /
                  <a href="https://arxiv.org/abs/2401.02400">arXiv</a> /
                  <a href="https://github.com/3DAnimals/3DAnimals">Code</a> /
                  <a href="https://www.youtube.com/watch?v=w2f6yJARN1I">Video</a> /
                  <a href="https://huggingface.co/spaces/Kyle-Liz/3DFauna_demo">Demo</a>
                  <p></p>
                  <p>
                    3D-Fauna learns a pan-category deformable 3D model of more than 100 different animal species using only 2D Internet images as training data, without any prior shape models or keypoint annotations. At test time, the model can turn a single image of an quadruped instance into an articulated, textured 3D mesh in a feed-forward manner, ready for animation and rendering.
                  </p>
              </td>
          </tr> <!--li20243dfauna-->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
              <td style="padding:20px;width:35%;vertical-align:middle">
                <img align='middle' src="images/2024Zeronvs_Sargent.gif" alt="zeronvs" style="width:auto; height:auto; max-width:100%;">
                      </td>
              <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2310.17994"> <!--TODO add link-->
                      <papertitle>ZeroNVS: Zero-Shot 360-Degree View Synthesis from a Single Real Image</papertitle>
                  </a>
                  <br>
                  <a href="https://kylesargent.github.io/">Kyle Sargent</a>,
                  <strong>Zizhang Li</strong>,
                  <a>Tanmay Shah</a>, 
                  <a>Charles Herrmann</a>, 
                  <a href="https://kovenyu.com/">Hong-Xing Yu</a>, 
                  <a href="https://cs.stanford.edu/~yzzhang/">Yunzhi Zhang</a>, 
                  <a href="https://ericryanchan.github.io/">Eric Ryan Chan</a>, 
                  <a>Dmitry Lagun</a>, 
                  <a href="https://profiles.stanford.edu/fei-fei-li/">Li Fei-Fei</a>, 
                  <a href="https://deqings.github.io/">Deqing Sun</a>, 
                  <a href="https://jiajunwu.com/">Jiajun Wu</a>
                  <br>
                  <em>CVPR</em>, 2024
                  <br>
                  <a href="https://kylesargent.github.io/zeronvs/">Project page</a> /
                  <a href="https://kylesargent.github.io/zeronvs">arXiv</a> /
                  <a href="https://github.com/kylesargent/zeronvs">code</a>
                  <p></p>
                  <p>
                    We train a 3D-aware diffusion model, ZeroNVS on a mixture of scene data sources that capture object-centric, indoor, and outdoor scenes. 
                    This enables zero-shot SDS distillation of 360-degree NeRF scenes from a single image. 
                    Our model sets a new state-of-the-art result in LPIPS on the DTU dataset in the zero-shot setting. 
                    We also use the MipNeRF-360 dataset as a benchmark for single-image NVS.
                  </p>
              </td>
          </tr> <!--sargent2023zeronvs-->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="rico_stop()" onmouseover="rico_start()">
              <td style="padding:20px;width:35%;vertical-align:middle">
                <div class="one">
                  <div class="two" id = 'rico_shape'>
                    <img src='images/2023RICO_Li_after.png' width="250"></div>
                  <img src='images/2023RICO_Li_before.png' width="250"></div>
                  <script type="text/javascript">
                  function rico_start() { 
                  document.getElementById('rico_shape').style.opacity = "1";
                  }
                  function rico_stop() { 
                  document.getElementById('rico_shape').style.opacity = "0"; 
                  }
                  rico_stop()
                  </script>
                  </script>
                <!-- <img align='middle' src="images/2023RICO_Li.gif" alt="RICO" style="width:auto; height:auto; max-width:100%;"> -->
                <!-- <video autoplay loop muted playsinline width="320" height="100%" style="max-width: 100%;">
                  <source src="images/2023RICO_Li.mp4" type="video/mp4" alt="RICO">
                </video> -->
                      </td>
              <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2303.08605"> <!--TODO add link-->
                      <papertitle>RICO: Regularizing the Unobservable for Indoor Compositional Reconstruction</papertitle>
                  </a>
                  <br>
                  <strong>Zizhang Li</strong>,
                  <a href="https://shawlyu.github.io/">Xiaoyang Lyu</a>,
                  <a>Yuanyuan Ding</a>,
                  <a href="https://sallymmx.github.io/">Mengmeng Wang</a>,
                  <a href="https://yiyiliao.github.io/">Yiyi Liao&#8224</a>,
                  <a href="https://person.zju.edu.cn/yongliu">Yong Liu&#8224</a>
                  <br>
                  <em>ICCV</em>, 2023
                  <br>
                  <a href="https://arxiv.org/abs/2303.08605">arXiv</a> /
                  <a href="https://github.com/kyleleey/RICO">code</a>
                  <p></p>
                  <p>
                    We investigate the existing problems in SDF-based object compositional reconstruction under the partial observation, 
                    and propose different regularizations following the geometry prior to reach a clean and water-tight disentanglement.
                  </p>
              </td>
          </tr> <!--li2023rico-->

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:20px;width:35%;vertical-align:middle">
                    <img src='images/2023Occ-SDF_Lyu.gif' width="250" alt="occ-sdf" style="width:auto; height:auto; max-width:100%;">
                        </td>
                <td width="75%" valign="middle">
                    <a href="https://arxiv.org/abs/2303.09152"> <!--TODO add link-->
                        <papertitle>Learning a Room with the Occ-SDF Hybrid: Signed Distance Function Mingled with Occupancy Aids Scene Representation</papertitle>
                    </a>
                    <br>
                    <a href="https://shawlyu.github.io/">Xiaoyang Lyu</a>,
                    <a>Peng Dai</a>,
                    <strong>Zizhang Li</strong>,
                    <a>Dongyu Yan</a>,
                    <a>Yi Lin</a>,
                    <a href="https://www.eee.hku.hk/~evanpeng/">Yifan Peng</a>,
                    <a href="https://xjqi.github.io/">Xiaojuan Qi</a>
                    <br>
                    <em>ICCV</em>, 2023
                    <br>
                    <a href="https://shawlyu.github.io/Occ-SDF-Hybrid/">Project page</a> /
                    <a href="https://arxiv.org/abs/2303.09152">arXiv</a> /
                    <a href="https://github.com/shawLyu/Occ-SDF-Hybrid">code</a>
                    <p></p>
                    <p>
                      We study and analyze several key observations in indoor scene SDF-based volume rendering reconstruction methods. Upon those observations,
                      we push forward an Occ-SDF hybrid representation for better reconstruction performance.
                    </p>
                </td>
            </tr> <!--li2023rico-->
        
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
              <td style="padding:20px;width:35%;vertical-align:middle">
                  <img src='images/2023ViLG_xu.png' width="250" alt="vilg" style="width:auto; height:auto; max-width:100%;">
                      </td>
              <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2302.12610"> <!--TODO add link-->
                      <papertitle>A Joint Modeling of Vision-Language-Action for Target-oriented Grasping in Clutter</papertitle>
                  </a>
                  <br>
                  <a href="https://xukechun.github.io/">Kechun Xu</a>,
                  <a>Shuqi Zhao</a>,
                  <a>Zhongxiang Zhou</a>,
                  <strong>Zizhang Li</strong>,
                  <a>Huaijin Pi</a>,
                  <a href="https://zhuyifengzju.github.io/">Yifeng Zhu</a>,
                  <a href="https://ywang-zju.github.io/">Yue Wang</a>,
                  <a href="https://person.zju.edu.cn/0097062">Rong Xiong</a>
                  <br>
                  <em>ICRA</em>, 2023
                  <br>
                  <a href="https://arxiv.org/abs/2302.12610">arXiv</a> /
                  <a href="https://github.com/xukechun/Vision-Language-Grasping">code</a>
                  <p></p>
                  <p>
                    We propose to jointly model vision, language and action with object-centric representations for the task of
                    language-conditioned grasping in clutter.
                  </p>
              </td>
          </tr> <!--xu2023vilg-->

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:20px;width:35%;vertical-align:middle">
                    <img src='images/2023FMP_xu.png' width="250" alt="fmp" style="width:auto; height:auto; max-width:100%;">
                        </td>
                <td width="75%" valign="middle">
                    <a href="https://arxiv.org/abs/2302.13024">
                        <papertitle>Failure-aware Policy Learning for Self-assessable Robotics Tasks</papertitle>
                    </a>
                    <br>
                    <a href="https://xukechun.github.io/">Kechun Xu</a>,
                    <a href="https://www.rjchen.site/">Runjian Chen</a>,
                    <a>Shuqi Zhao</a>,
                    <strong>Zizhang Li</strong>,
                    <a>Hongxiang Yu</a>,
                    <a>Ci Chen</a>,
                    <a href="https://ywang-zju.github.io/">Yue Wang</a>,
                    <a href="https://person.zju.edu.cn/0097062">Rong Xiong</a>
                    <br>
                    <em>ICRA</em>, 2023
                    <br>
                    <a href="https://arxiv.org/abs/2302.13024">arXiv</a>
                    <p></p>
                    <p>
                      We investigate the dependency between the self-assessment results and remaining actions by learning the
                      failure-aware policy, and propose two policy architectures.
                    </p>
                </td>
            </tr> <!--xu2023fmp-->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
              <td style="padding:20px;width:35%;vertical-align:middle">
                  <img src='images/2022_ENeRV_li.png' width="250" alt="enerv" style="width:auto; height:auto; max-width:100%;">
                      </td>
              <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2207.08132">
                      <papertitle>E-NeRV: Expedite Neural Video Representation with Disentangled Spatial-Temporal Context</papertitle>
                  </a>
                  <br>
                  <strong>Zizhang Li</strong>,
                  <a href="https://sallymmx.github.io/">Mengmeng Wang</a>,
                  <a>Huaijin Pi</a>,
                  <a href="https://xukechun.github.io/">Kechun Xu</a>,
                  <a>Jianbiao Mei</a>,
                  <a href="https://person.zju.edu.cn/yongliu">Yong Liu</a>
                  <br>
                  <em>ECCV</em>, 2022
                  <br>
                  <a href="https://arxiv.org/abs/2207.08132">arXiv</a> /
                  <a href="https://github.com/kyleleey/E-NeRV">code</a>
                  <p></p>
                  <p>
                    We investigate the architecture of frame-wise implicit neural video representation and upgrade it by removing a large portion of redundant parameters, and re-design
                    the network architecture following a spatial-temporal disentanglement motivation.
                  </p>
              </td>
          </tr> 
          <!--li2022ENeRV-->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
              <td style="padding:20px;width:35%;vertical-align:middle">
                  <img src='images/2021CGPart_liu.png' width="250" alt="cgpart" style="width:auto; height:auto; max-width:100%;">
                      </td>
              <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2103.14098">
                      <papertitle>Learning Part Segmentation through Unsupervised Domain Adaptation from Synthetic Vehicles</papertitle>
                  </a>
                  <br>
                  <a> Qing Liu</a>,
                  <a href="https://adamkortylewski.com/">Adam Kortylewski</a>,
                  <a href="https://zhishuai.xyz/">Zhishuai Zhang</a>,
                  <strong>Zizhang Li</strong>,
                  <a>Mengqi Guo</a>,
                  <a>Qihao Liu</a>,
                  <a>Xiaoding Yuan</a>,
                  <a href="https://jitengmu.github.io/">Jiteng Mu</a>,
                  <a href="https://weichaoqiu.com/">Weichao Qiu</a>,
                  <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>
                  <br>
                  <em>CVPR</em>, 2022, 
                      <font color="red">oral</font>
                  <br>
                  <a href="https://arxiv.org/abs/2103.14098">arXiv</a> /
                  <a href="https://github.com/qliu24/udapart/blob/gh-pages/index.md">code</a>
                  <p></p>
                  <p>
                    We construct a synthetic multi-part dataset with different categories of objects, 
                    evaluate different part segmentation UDA methods with this benchmark, and also provide an improved baseline.
                  </p>
              </td>
          </tr> <!--liu2022CGPart-->

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:20px;width:35%;vertical-align:middle">
                    <img src='images/2021MaIL_li.png' width="250" alt="mail" style="width:auto; height:auto; max-width:100%;">
                        </td>
                <td width="75%" valign="middle">
                    <a href="https://arxiv.org/abs/2111.10747">
                        <papertitle>MaIL: A Unified Mask-Image-Language Trimodal Network for Referring Image Segmentation</papertitle>
                    </a>
                    <br>
                    <strong>Zizhang Li*</strong>,
                    <a href="https://sallymmx.github.io/">Mengmeng Wang*</a>,
                    <a>Jianbiao Mei</a>,
                    <a href="https://person.zju.edu.cn/yongliu">Yong Liu</a>
                    <br>
                    <em>arxiv</em>, 2021
                    <br>
                    <a href="https://arxiv.org/abs/2111.10747">arXiv</a>
                    <p></p>
                    <p>
                      We propose to regard the binary mask as a unique modality and train the tri-modal embedding space 
                      on top of ViLT for referring segmentation task.
                    </p>
                </td>
            </tr> <!--li2021MaIL-->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
              <td style="padding:20px;width:35%;vertical-align:middle">
                  <img src='images/2021TrioNet_pi.png' width="250" alt="trionet" style="width:auto; height:auto; max-width:100%;">
                      </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2111.07547">
                  <papertitle>Searching for TrioNet: Combining Convolution with Local and Global Self-Attention</papertitle>
                </a>
                  <br>
                  <a>Huaijin Pi</a>,
                  <a href="https://csrhddlam.github.io/">Huiyu Wang</a>,
                  <a href="https://yingwei.li/">Yingwei Li</a>,
                  <strong>Zizhang Li</strong>,
                  <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>
                  <br>
                  <em>BMVC</em>, 2021
                  <br>
                  <a href="https://arxiv.org/abs/2111.07547">arXiv</a> /
                  <a href="https://github.com/phj128/TrioNet">code</a>
                  <p></p>
                  <p>
                    We propose a weight-sharing NAS method to combine convolution, local and global self-attention operators.
                  </p>
              </td>
          </tr> <!--2021trio_pi-->

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:20px;width:35%;vertical-align:middle">
                    <img src='images/2021PAPLoss_tao.png' width="250" alt="paploss" style="width:auto; height:auto; max-width:100%;">
                        </td>
                <td width="75%" valign="middle">
                    <a>
                      <papertitle>Searching Parameterized AP Loss for Object Detection</papertitle>
                    </a>
                    <br>
                    <a>Chenxin Tao*</a>,
                    <strong>Zizhang Li*</strong>,
                    <a>Xizhou Zhu</a>,
                    <a href="http://www.gaohuang.net/">Gao Huang</a>,
                    <a href="https://person.zju.edu.cn/yongliu">Yong Liu</a>,
                    <a href="https://jifengdai.org/">Jifeng Dai</a>
                    <br>
                    <em>NeurIPS</em>, 2021
                    <br>
                    <a href="https://arxiv.org/abs/2112.05138">arXiv</a> /
                    <a href="https://github.com/fundamentalvision/Parameterized-AP-Loss">code</a>
                    <p></p>
                    <p>
                      We transform the non-diffrentiable AP metric to differentiable loss function by utilizing Bezier curve parameterization. We further
                      use PPO to search the parameters and show improved performance of the PAP loss on various detectors.
                    </p>
                </td>
            </tr> <!--2021paploss_tao-->

          </tbody></table>

          <hr>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                    <heading>Services</heading>
                </td>
            </tr>
          </tbody></table>
  
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <ul>
                <li>Reviewer of <b>3DV</b>, <b>AAAI</b>, <b>BMVC</b>, <b>CAI</b>, <b>CVPR</b>, <b>ECCV</b>, <b>ICCV</b>, <b>ICLR</b>, <b>ICML</b>, <b>NeurIPS</b>.</li>
                <li>OpenReview Chair of <a href="https://3dvconf.github.io/2025/people/">3DV2025</a>.</li>
              </ul>
            </td>
          </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;">This website is adapted from <a href="https://jonbarron.info/">this amazing template</a></p>
              </td>
            </tr>
          </tbody></table>
        
      </td>
    </tr>
    <tr style="padding:400px">
      <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&t=tt&d=ep0TN0aIPm93BpeJ4rN00EItybZktlMsgiKfYA3BA7Q'></script>
    </tr>

  </table>

<!-- Default Statcounter code for Personal https://kyleleey.github.io/ -->
<script type="text/javascript">
  var sc_project=13105055; 
  var sc_invisible=1; 
  var sc_security="0831f084"; 
  </script>
  <script type="text/javascript"
  src="https://www.statcounter.com/counter/counter.js" async></script>
  <noscript><div class="statcounter"><a title="Web Analytics Made Easy -
  Statcounter" href="https://statcounter.com/" target="_blank"><img
  class="statcounter" src="https://c.statcounter.com/13105055/0/0831f084/1/"
  alt="Web Analytics Made Easy - Statcounter"
  referrerPolicy="no-referrer-when-downgrade"></a></div></noscript>
  <!-- End of Statcounter Code -->

</body>

</html>
